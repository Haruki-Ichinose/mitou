{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4538270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 0. Setup\n",
    "# ===========================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "BASE_02 = \"/workspace/data/02/players\"\n",
    "BASE_03 = \"/workspace/data/03/dynamic\"\n",
    "BASE_03_STATIC = \"/workspace/data/03/static\"  # â˜… static å‡ºåŠ›\n",
    "os.makedirs(BASE_03, exist_ok=True)\n",
    "\n",
    "DYNAMIC_CONFIG = {\n",
    "    \"seq_len\": 30,\n",
    "    \"break_days\": 30,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"latent_dim\": 32,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 32,\n",
    "    \"reset_days\": 7,\n",
    "\n",
    "    # â˜… S1: sigmaé–¾å€¤ã‚’ã‚„ã‚ã¦åˆ†ä½ç‚¹é–¾å€¤ã«ã™ã‚‹ï¼ˆé ‘å¥ï¼‰\n",
    "    \"threshold_q\": 0.99,\n",
    "\n",
    "    # å­¦ç¿’ã«ä½¿ã†æœ€ä½ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°ï¼ˆå°‘ãªã™ãã‚‹ã¨ä¸å®‰å®šï¼‰\n",
    "    \"min_train_seqs\": 50,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "META_EXCLUDE = [\"athlete_id\", \"athlete_name\", \"date_\", \"md_offset\", \"md_phase\", \"is_match_day\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1425bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 1. Model & Dataset\n",
    "# ===========================================\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, seq_array):\n",
    "        self.seq_array = seq_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.seq_array[idx], dtype=torch.float32)\n",
    "\n",
    "\n",
    "class LSTMAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_dim, hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder_fc = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.decoder_fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder = nn.LSTM(\n",
    "            hidden_dim, input_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_out, _ = self.encoder(x)\n",
    "        h_last = enc_out[:, -1, :]\n",
    "        z = self.encoder_fc(h_last)\n",
    "\n",
    "        dec_init = self.decoder_fc(z).unsqueeze(1)\n",
    "        dec_init = dec_init.repeat(1, x.size(1), 1)\n",
    "\n",
    "        dec_out, _ = self.decoder(dec_init)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3673b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 2. Dynamic preprocessing\n",
    "# ===========================================\n",
    "def add_season_block(df_dyn: pd.DataFrame, break_days: int) -> pd.DataFrame:\n",
    "    df_dyn = df_dyn.sort_values(\"date_\").reset_index(drop=True)\n",
    "    dates = pd.to_datetime(df_dyn[\"date_\"])\n",
    "    gap_days = dates.diff().dt.days\n",
    "    season_block = (gap_days > break_days).cumsum().fillna(0).astype(int)\n",
    "    df_dyn[\"season_block\"] = season_block.values\n",
    "    return df_dyn\n",
    "\n",
    "\n",
    "def add_season_reset_zone(df_dyn: pd.DataFrame, reset_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    season_block ã®åˆ‡ã‚Šæ›¿ã‚ã‚Šç›´å¾Œ reset_days ã‚’ 1 ã«ã™ã‚‹\n",
    "    \"\"\"\n",
    "    df_dyn = df_dyn.copy()\n",
    "    df_dyn[\"season_reset_zone\"] = 0\n",
    "    for b, sub in df_dyn.groupby(\"season_block\"):\n",
    "        if b == 0:\n",
    "            continue\n",
    "        start = pd.to_datetime(sub[\"date_\"]).min()\n",
    "        end = start + pd.Timedelta(days=reset_days - 1)\n",
    "        mask = (pd.to_datetime(df_dyn[\"date_\"]) >= start) & (pd.to_datetime(df_dyn[\"date_\"]) <= end)\n",
    "        df_dyn.loc[mask, \"season_reset_zone\"] = 1\n",
    "    return df_dyn\n",
    "\n",
    "\n",
    "def load_static_flags(athlete_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    static_labels.parquet ã‹ã‚‰ date_ ã”ã¨ã® static_anomaly ã‚’èª­ã‚€\n",
    "    ç„¡ã‘ã‚Œã° None\n",
    "    \"\"\"\n",
    "    p = os.path.join(BASE_03_STATIC, str(athlete_id), \"static_labels.parquet\")\n",
    "    if not os.path.exists(p):\n",
    "        return None\n",
    "    s = pd.read_parquet(p)\n",
    "    s[\"date_\"] = pd.to_datetime(s[\"date_\"])\n",
    "    return s[[\"date_\", \"static_anomaly\"]]\n",
    "\n",
    "\n",
    "def scale_per_block(df_dyn: pd.DataFrame, feature_cols: list):\n",
    "    \"\"\"\n",
    "    season_block ã”ã¨ã« StandardScaler ã§ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "    + backendç”¨ã« scaler params ã‚’ä¿å­˜ã§ãã‚‹ã‚ˆã†ã«è¿”ã™\n",
    "    \"\"\"\n",
    "    X_raw = df_dyn[feature_cols].astype(float).values\n",
    "    X_scaled = np.zeros_like(X_raw)\n",
    "\n",
    "    scaler_params = {}  # block -> {mean, scale}\n",
    "    for b in sorted(df_dyn[\"season_block\"].unique()):\n",
    "        mask = (df_dyn[\"season_block\"].values == b)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled[mask] = scaler.fit_transform(X_raw[mask])\n",
    "\n",
    "        scaler_params[int(b)] = {\n",
    "            \"mean\": scaler.mean_.tolist(),\n",
    "            \"scale\": scaler.scale_.tolist(),\n",
    "        }\n",
    "\n",
    "    return X_scaled, scaler_params\n",
    "\n",
    "\n",
    "def build_sequences(X_scaled: np.ndarray, dates: np.ndarray, seq_len: int):\n",
    "    \"\"\"\n",
    "    å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨è«–ç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œã‚‹ï¼ˆseq_datesã¯çª“æœ«å°¾æ—¥ï¼‰\n",
    "    \"\"\"\n",
    "    seq_list, seq_dates, end_idx = [], [], []\n",
    "    if len(X_scaled) < seq_len:\n",
    "        return None, None, None\n",
    "\n",
    "    for i in range(seq_len - 1, len(X_scaled)):\n",
    "        seq_list.append(X_scaled[i - seq_len + 1:i + 1])\n",
    "        seq_dates.append(dates[i])\n",
    "        end_idx.append(i)\n",
    "\n",
    "    return np.stack(seq_list), np.array(seq_dates), np.array(end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c59cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 3. Dynamic anomaly (single player)\n",
    "# ===========================================\n",
    "def run_dynamic_anomaly_single_player(df_dyn: pd.DataFrame, athlete_id: str, config: dict):\n",
    "\n",
    "    df_dyn = df_dyn.copy()\n",
    "    df_dyn[\"date_\"] = pd.to_datetime(df_dyn[\"date_\"])\n",
    "    df_dyn = add_season_block(df_dyn, break_days=config[\"break_days\"])\n",
    "    df_dyn = add_season_reset_zone(df_dyn, reset_days=config[\"reset_days\"])\n",
    "\n",
    "    # â˜… S2: static flags mergeï¼ˆç„¡ã‘ã‚Œã° 0 æ‰±ã„ï¼‰\n",
    "    static_flags = load_static_flags(athlete_id)\n",
    "    if static_flags is not None:\n",
    "        df_dyn = df_dyn.merge(static_flags, on=\"date_\", how=\"left\")\n",
    "        df_dyn[\"static_anomaly\"] = df_dyn[\"static_anomaly\"].fillna(0).astype(int)\n",
    "    else:\n",
    "        df_dyn[\"static_anomaly\"] = 0\n",
    "\n",
    "    # ç‰¹å¾´é‡åˆ—\n",
    "    feature_cols = [c for c in df_dyn.columns if c not in META_EXCLUDE + [\"season_block\", \"season_reset_zone\", \"static_anomaly\"]]\n",
    "    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df_dyn[c])]\n",
    "\n",
    "    if len(feature_cols) == 0:\n",
    "        return None, None\n",
    "\n",
    "    # æ¬ æï¼šã¾ãšã¯ median ã§åŸ‹ã‚ã‚‹ï¼ˆAEã®å…¥åŠ›ãŒNaNã ã¨å£Šã‚Œã‚‹ï¼‰\n",
    "    impute_median = df_dyn[feature_cols].median()\n",
    "    df_dyn[feature_cols] = df_dyn[feature_cols].fillna(impute_median)\n",
    "\n",
    "    # ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã‚¹ã‚±ãƒ¼ãƒ« + paramsä¿å­˜\n",
    "    X_scaled, scaler_params = scale_per_block(df_dyn, feature_cols)\n",
    "    dates = df_dyn[\"date_\"].values\n",
    "\n",
    "    # æ¨è«–ç”¨ï¼šå…¨æ—¥ã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ\n",
    "    seq_all, seq_dates, end_idx = build_sequences(X_scaled, dates, seq_len=config[\"seq_len\"])\n",
    "    if seq_all is None or len(seq_all) < 20:\n",
    "        return None, None\n",
    "\n",
    "    # â˜… å­¦ç¿’ç”¨ï¼šend_idx ã«å¯¾å¿œã™ã‚‹æ—¥ä»˜ã®ãƒ•ãƒ©ã‚°ã§é™¤å¤–ï¼ˆreset_zone, static_anomalyï¼‰\n",
    "    end_rows = df_dyn.iloc[end_idx].reset_index(drop=True)\n",
    "    train_mask = (end_rows[\"season_reset_zone\"].values == 0) & (end_rows[\"static_anomaly\"].values == 0)\n",
    "\n",
    "    seq_train = seq_all[train_mask]\n",
    "    if len(seq_train) < config.get(\"min_train_seqs\", 50):\n",
    "        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "        return None, None\n",
    "\n",
    "    # Dataset / Loader\n",
    "    dataset = SequenceDataset(seq_train)\n",
    "    train_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # reproducibility (è»½ã)\n",
    "    torch.manual_seed(config.get(\"random_state\", 42))\n",
    "    np.random.seed(config.get(\"random_state\", 42))\n",
    "\n",
    "    model = LSTMAutoEncoder(\n",
    "        input_dim=seq_all.shape[2],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        latent_dim=config[\"latent_dim\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dropout=config[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # ---- train ----\n",
    "    for _ in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        for x in train_loader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(x)\n",
    "            loss = criterion(recon, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # ---- reconstruction error for ALL sequences ----\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(seq_all)):\n",
    "            x = torch.tensor(seq_all[i:i+1], dtype=torch.float32).to(device)\n",
    "            recon = model(x)\n",
    "            err = ((recon - x) ** 2).mean().item()\n",
    "            errors.append(err)\n",
    "    errors = np.array(errors)\n",
    "\n",
    "    df_seq = pd.DataFrame({\n",
    "        \"date_\": pd.to_datetime(seq_dates),\n",
    "        \"dyn_error\": errors,\n",
    "    })\n",
    "\n",
    "    # join flags (reset/static) for the end-date rows\n",
    "    df_flags = end_rows[[\"date_\", \"season_block\", \"season_reset_zone\", \"static_anomaly\"]].copy()\n",
    "    df_flags[\"date_\"] = pd.to_datetime(df_flags[\"date_\"])\n",
    "    df_seq = df_seq.merge(df_flags, on=\"date_\", how=\"left\")\n",
    "\n",
    "    # â˜… S1: threshold by quantile on VALID errors (not reset, not static anomaly)\n",
    "    valid_err = df_seq.loc[\n",
    "        (df_seq[\"season_reset_zone\"] == 0) & (df_seq[\"static_anomaly\"] == 0),\n",
    "        \"dyn_error\"\n",
    "    ]\n",
    "\n",
    "    thr = float(np.quantile(valid_err.values, config[\"threshold_q\"]))\n",
    "    df_seq[\"dyn_thr\"] = thr\n",
    "\n",
    "    df_seq[\"dyn_anomaly\"] = (\n",
    "        (df_seq[\"dyn_error\"] > thr) &\n",
    "        (df_seq[\"season_reset_zone\"] == 0)\n",
    "    ).astype(int)\n",
    "\n",
    "    # æŒç¶šæŒ‡æ¨™ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã§å¼·ã„ï¼‰\n",
    "    df_seq = df_seq.sort_values(\"date_\").reset_index(drop=True)\n",
    "    # streak: é€£ç¶šç•°å¸¸æ—¥æ•°\n",
    "    streak = []\n",
    "    cur = 0\n",
    "    for a in df_seq[\"dyn_anomaly\"].values:\n",
    "        if a == 1:\n",
    "            cur += 1\n",
    "        else:\n",
    "            cur = 0\n",
    "        streak.append(cur)\n",
    "    df_seq[\"dyn_streak\"] = streak\n",
    "\n",
    "    # â˜… S3: artifacts\n",
    "    artifacts = {\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"impute_median\": {k: float(v) if pd.notna(v) else None for k, v in impute_median.to_dict().items()},\n",
    "        \"scaler_params_per_block\": scaler_params,\n",
    "        \"threshold_q\": config[\"threshold_q\"],\n",
    "        \"dyn_thr\": thr,\n",
    "        \"config\": config,\n",
    "    }\n",
    "\n",
    "    return df_seq, artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6e7281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯¾è±¡é¸æ‰‹æ•°: 30\n",
      "âœ… saved dynamic anomaly â†’ 121b05df-f5f6-4029-92a7-5420dea45e4d\n",
      "âœ… saved dynamic anomaly â†’ 13bb34b4-8c38-4c86-86b8-bbe8574988c8\n",
      "âœ… saved dynamic anomaly â†’ 15d36f96-6a91-4787-96f8-5fdf8565006b\n",
      "âœ… saved dynamic anomaly â†’ 223a7cbc-a76b-4e36-ab5c-215fc9492e84\n",
      "âœ… saved dynamic anomaly â†’ 3ded61ff-c67b-4776-a1ef-5050bb5c7fd3\n",
      "âœ… saved dynamic anomaly â†’ 44eea4b6-3614-4ca2-b8d7-098b6120c1fb\n",
      "âœ… saved dynamic anomaly â†’ 45f771a4-dab1-4e2a-8f92-cd219b677ab9\n",
      "âœ… saved dynamic anomaly â†’ 4759d9d8-9e0e-44b8-9c70-874bf974c4ba\n",
      "âœ… saved dynamic anomaly â†’ 68341b43-2561-4972-95f4-341f8530e023\n",
      "âœ… saved dynamic anomaly â†’ 6eda50d0-970c-44ab-b470-de9ebc71ae52\n",
      "âœ… saved dynamic anomaly â†’ 714e5fd4-609c-4e96-bea6-b7f3966bf681\n",
      "âœ… saved dynamic anomaly â†’ 7995a06b-bb0d-4343-bc18-cbdf623d4640\n",
      "âœ… saved dynamic anomaly â†’ 83761ac5-fbd3-4422-ba30-b6b4915da945\n",
      "âœ… saved dynamic anomaly â†’ 83a85906-44bd-4976-8906-53faed1684f3\n",
      "âœ… saved dynamic anomaly â†’ 854ad249-d0e7-4e32-bdab-adde8740c980\n",
      "âœ… saved dynamic anomaly â†’ 8fdd27ba-fabe-4df0-a4ec-a53e7cda8383\n",
      "âœ… saved dynamic anomaly â†’ 923787b2-19ea-4cf2-ace3-d9680d482cfb\n",
      "âœ… saved dynamic anomaly â†’ 95662868-6c2b-4c84-8c38-f31685f9a350\n",
      "âœ… saved dynamic anomaly â†’ 9a1d2d89-e1f6-4984-8a33-aa7f2473e5d7\n",
      "âœ… saved dynamic anomaly â†’ a231087b-4ba6-43a3-b96a-8d2d912cf7e0\n",
      "âœ… saved dynamic anomaly â†’ a2562343-249b-4971-94fd-0c17b6b38e52\n",
      "âœ… saved dynamic anomaly â†’ c371d2b9-a467-4bd2-8c69-42ef79527dde\n",
      "âœ… saved dynamic anomaly â†’ c77b7e70-5c83-47a3-ab39-e9165aa3e586\n",
      "âœ… saved dynamic anomaly â†’ d35e1edc-5a93-4c2a-bd6b-a4fbe25297aa\n",
      "âœ… saved dynamic anomaly â†’ d5f7cf9f-2d74-4bed-96a5-0c2650a2cdfc\n",
      "âœ… saved dynamic anomaly â†’ d605c7e5-a678-44ce-8bd9-fa66594edd49\n",
      "âœ… saved dynamic anomaly â†’ e5f3d419-abf1-4633-9e63-300e4c10e13d\n",
      "âœ… saved dynamic anomaly â†’ ead0a333-1456-4ce1-9407-1787bf0b6760\n",
      "âœ… saved dynamic anomaly â†’ f0dfd8b9-7cb1-4a3a-991a-4fb9b1e605d3\n",
      "âœ… saved dynamic anomaly â†’ f38be9da-a872-495a-8d3e-626f9c75ae89\n",
      "ğŸ‰ Dynamic anomaly (all players) completed\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# 4. Run for ALL players\n",
    "# ===========================================\n",
    "athlete_dirs = sorted([d for d in os.listdir(BASE_02) if os.path.isdir(os.path.join(BASE_02, d))])\n",
    "print(\"å¯¾è±¡é¸æ‰‹æ•°:\", len(athlete_dirs))\n",
    "\n",
    "for athlete_id in athlete_dirs:\n",
    "\n",
    "    dyn_path = os.path.join(BASE_02, athlete_id, f\"{athlete_id}_dynamic.parquet\")\n",
    "    if not os.path.exists(dyn_path):\n",
    "        continue\n",
    "\n",
    "    df_dyn = pd.read_parquet(dyn_path)\n",
    "\n",
    "    res, artifacts = run_dynamic_anomaly_single_player(df_dyn, athlete_id=str(athlete_id), config=DYNAMIC_CONFIG)\n",
    "    if res is None:\n",
    "        continue\n",
    "\n",
    "    res[\"athlete_id\"] = athlete_id\n",
    "    res[\"method\"] = \"LSTM_AE\"\n",
    "    res[\"params\"] = json.dumps(DYNAMIC_CONFIG, ensure_ascii=False)\n",
    "\n",
    "    out_dir = os.path.join(BASE_03, str(athlete_id))\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_path = os.path.join(out_dir, \"dynamic_labels.parquet\")\n",
    "    res.to_parquet(out_path, index=False)\n",
    "\n",
    "    art_path = os.path.join(out_dir, \"dynamic_artifacts.json\")\n",
    "    with open(art_path, \"w\") as f:\n",
    "        json.dump(artifacts, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… saved dynamic anomaly â†’ {athlete_id}\")\n",
    "\n",
    "print(\"ğŸ‰ Dynamic anomaly (all players) completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ce3071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded dynamic results\n",
      "Total rows: 10324\n",
      "Players   : 30\n",
      "\n",
      "=== [1] Overall dyn_anomaly counts ===\n",
      "dyn_anomaly\n",
      "0    10200\n",
      "1      124\n",
      "Name: count, dtype: int64\n",
      "Overall anomaly rate: 0.012\n",
      "\n",
      "=== [2] Player-wise anomaly rate (describe) ===\n",
      "count    30.000000\n",
      "mean      0.012721\n",
      "std       0.002364\n",
      "min       0.009756\n",
      "25%       0.010639\n",
      "50%       0.012180\n",
      "75%       0.014406\n",
      "max       0.018519\n",
      "Name: dyn_anomaly, dtype: float64\n",
      "\n",
      "=== [3] Max dyn_streak per player (TOP 5) ===\n",
      "athlete_id\n",
      "714e5fd4-609c-4e96-bea6-b7f3966bf681    6\n",
      "d5f7cf9f-2d74-4bed-96a5-0c2650a2cdfc    5\n",
      "4759d9d8-9e0e-44b8-9c70-874bf974c4ba    5\n",
      "9a1d2d89-e1f6-4984-8a33-aa7f2473e5d7    5\n",
      "121b05df-f5f6-4029-92a7-5420dea45e4d    4\n",
      "Name: dyn_streak, dtype: int64\n",
      "\n",
      "=== [4] dyn_thr distribution ===\n",
      "count    10324.000000\n",
      "mean         2.424251\n",
      "std          1.292441\n",
      "min          1.061228\n",
      "25%          1.337830\n",
      "50%          2.118569\n",
      "75%          2.623135\n",
      "max          4.980005\n",
      "Name: dyn_thr, dtype: float64\n",
      "\n",
      "=== [5] Sample anomaly rows ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_</th>\n",
       "      <th>dyn_error</th>\n",
       "      <th>season_block</th>\n",
       "      <th>season_reset_zone</th>\n",
       "      <th>static_anomaly</th>\n",
       "      <th>dyn_thr</th>\n",
       "      <th>dyn_anomaly</th>\n",
       "      <th>dyn_streak</th>\n",
       "      <th>athlete_id</th>\n",
       "      <th>method</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>2025-01-20</td>\n",
       "      <td>5.259851</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.980005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>121b05df-f5f6-4029-92a7-5420dea45e4d</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>2025-01-21</td>\n",
       "      <td>5.243083</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.980005</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>121b05df-f5f6-4029-92a7-5420dea45e4d</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>5.263999</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.980005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>121b05df-f5f6-4029-92a7-5420dea45e4d</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>2025-01-25</td>\n",
       "      <td>5.129557</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.980005</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>121b05df-f5f6-4029-92a7-5420dea45e4d</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>2025-02-11</td>\n",
       "      <td>4.982769</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.980005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>121b05df-f5f6-4029-92a7-5420dea45e4d</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2025-02-12</td>\n",
       "      <td>5.043195</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.980005</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>121b05df-f5f6-4029-92a7-5420dea45e4d</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>5.160212</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.980005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>121b05df-f5f6-4029-92a7-5420dea45e4d</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8239</th>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>1.061552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.061228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13bb34b4-8c38-4c86-86b8-bbe8574988c8</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>1.069523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.061228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13bb34b4-8c38-4c86-86b8-bbe8574988c8</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8444</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>1.127281</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.061228</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13bb34b4-8c38-4c86-86b8-bbe8574988c8</td>\n",
       "      <td>LSTM_AE</td>\n",
       "      <td>{\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_  dyn_error  season_block  season_reset_zone  static_anomaly  \\\n",
       "2168 2025-01-20   5.259851             2                  0               1   \n",
       "2169 2025-01-21   5.243083             2                  0               0   \n",
       "2170 2025-01-24   5.263999             2                  0               0   \n",
       "2171 2025-01-25   5.129557             2                  0               0   \n",
       "2184 2025-02-11   4.982769             2                  0               0   \n",
       "2185 2025-02-12   5.043195             2                  0               0   \n",
       "2186 2025-02-13   5.160212             2                  0               0   \n",
       "8239 2023-02-22   1.061552             0                  0               0   \n",
       "8443 2023-11-30   1.069523             0                  0               0   \n",
       "8444 2023-12-01   1.127281             0                  0               0   \n",
       "\n",
       "       dyn_thr  dyn_anomaly  dyn_streak                            athlete_id  \\\n",
       "2168  4.980005            1           1  121b05df-f5f6-4029-92a7-5420dea45e4d   \n",
       "2169  4.980005            1           2  121b05df-f5f6-4029-92a7-5420dea45e4d   \n",
       "2170  4.980005            1           3  121b05df-f5f6-4029-92a7-5420dea45e4d   \n",
       "2171  4.980005            1           4  121b05df-f5f6-4029-92a7-5420dea45e4d   \n",
       "2184  4.980005            1           1  121b05df-f5f6-4029-92a7-5420dea45e4d   \n",
       "2185  4.980005            1           2  121b05df-f5f6-4029-92a7-5420dea45e4d   \n",
       "2186  4.980005            1           3  121b05df-f5f6-4029-92a7-5420dea45e4d   \n",
       "8239  1.061228            1           1  13bb34b4-8c38-4c86-86b8-bbe8574988c8   \n",
       "8443  1.061228            1           1  13bb34b4-8c38-4c86-86b8-bbe8574988c8   \n",
       "8444  1.061228            1           2  13bb34b4-8c38-4c86-86b8-bbe8574988c8   \n",
       "\n",
       "       method                                             params  \n",
       "2168  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "2169  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "2170  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "2171  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "2184  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "2185  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "2186  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "8239  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "8443  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  \n",
       "8444  LSTM_AE  {\"seq_len\": 30, \"break_days\": 30, \"hidden_dim\"...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Dynamic anomaly : æœ€å°ãƒã‚§ãƒƒã‚¯ã‚»ãƒƒãƒˆ\n",
    "# ===========================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_03 = \"/workspace/data/03/dynamic\"\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1. å…¨é¸æ‰‹ã® dynamic_labels ã‚’èª­ã¿è¾¼ã¿\n",
    "# -------------------------------------------\n",
    "dfs = []\n",
    "\n",
    "for athlete_id in os.listdir(BASE_03):\n",
    "    p = os.path.join(BASE_03, athlete_id, \"dynamic_labels.parquet\")\n",
    "    if os.path.exists(p):\n",
    "        df = pd.read_parquet(p)\n",
    "        df[\"athlete_id\"] = athlete_id\n",
    "        dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"âœ… Loaded dynamic results\")\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Players   :\", df_all[\"athlete_id\"].nunique())\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. å…¨ä½“ã®ç•°å¸¸ç‡ï¼ˆã¾ãšå£Šã‚Œã¦ãªã„ã‹ï¼‰\n",
    "# -------------------------------------------\n",
    "print(\"\\n=== [1] Overall dyn_anomaly counts ===\")\n",
    "print(df_all[\"dyn_anomaly\"].value_counts())\n",
    "print(\"Overall anomaly rate:\",\n",
    "      df_all[\"dyn_anomaly\"].mean().round(4))\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 3. é¸æ‰‹åˆ¥ã®ç•°å¸¸ç‡åˆ†å¸ƒï¼ˆstaticã¨åŒã˜ï¼‰\n",
    "# -------------------------------------------\n",
    "player_rates = (\n",
    "    df_all\n",
    "    .groupby(\"athlete_id\")[\"dyn_anomaly\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "print(\"\\n=== [2] Player-wise anomaly rate (describe) ===\")\n",
    "print(player_rates.describe())\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4. dyn_streak ã®æœ€å¤§å€¤ï¼ˆé€£ç¶šç•°å¸¸ã®ç¢ºèªï¼‰\n",
    "# -------------------------------------------\n",
    "max_streak = (\n",
    "    df_all\n",
    "    .groupby(\"athlete_id\")[\"dyn_streak\"]\n",
    "    .max()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== [3] Max dyn_streak per player (TOP 5) ===\")\n",
    "print(max_streak.head(5))\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 5. dyn_thr ã®åˆ†å¸ƒï¼ˆæ¥µç«¯å€¤ãƒã‚§ãƒƒã‚¯ï¼‰\n",
    "# -------------------------------------------\n",
    "print(\"\\n=== [4] dyn_thr distribution ===\")\n",
    "print(df_all[\"dyn_thr\"].describe())\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 6. å‚è€ƒï¼šç•°å¸¸æ—¥ã®ã¿ã‚’è¦‹ã‚‹ï¼ˆç›®è¦–ç”¨ï¼‰\n",
    "# -------------------------------------------\n",
    "print(\"\\n=== [5] Sample anomaly rows ===\")\n",
    "display(\n",
    "    df_all[df_all[\"dyn_anomaly\"] == 1]\n",
    "    .sort_values([\"athlete_id\", \"date_\"])\n",
    "    .head(10)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
